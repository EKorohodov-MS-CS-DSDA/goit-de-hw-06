# Homework #6

## Опис Послідовності виконання домашнього завдання

### 1. Створення топіків
Для створення топіків використовується старий адаптований скрипт *create_topics.py*. В результаті виконання якого створюються два топіки: 
1. "Вхідний" *building_sensors* - куди потрапляють дані з симулятора роботи датчиків.
2. "Вихідний" *alerts* - куди потрапляють агреговані опрацьовані дані.

### 2. Генерація даних
Для генерації даних використовуються скрипт *producer.py*. Він генерує випадкові дані для симуляції роботи датчиків та відправляє їх разом з часовою міткою у вхідний топік використовуючи *KafkaProducer*. Використовується діапазон значень **[20; 50]** для температури та **[10; 90]** для значень вологості.

![alt text](https://github.com/EKorohodov-MS-CS-DSDA/goit-de-hw-06/blob/main/p1_Screenshot.jpg)

### 3. Опрацювання даних
Для опрацювання, агрегації та запису в потік даних використовується скрипт *processor.py*.
Основні кроки, що виконуються щодо опрацювання даних:
1. Читання даних:
- Створюється *Spark* сесія
- Відбувається читання потока з вхідного топіку.
- Визначається схема вхадних даних.
- Відбувається читання значень обмежень з таблиці *alerts_conditions.csv*.
2. Агрегація даних
- Вхідні дані агрегуються використовуючи watermark 10 секунд та групуються за ковзними вікнами довжиною 1 хвилина та ковзним інтервалом 30 секунд.
- Показники температури та вологості об'єднуються у **t_avg** та **h_avg** відповідно.
- Відбувається фільтрація даних за допомогою *cross join*-а зі значеннями з таблиці обмежень.
3. Відправка даних
- Формується набір даних що відповідає вихідним очікуванням.
- Дані у вигляді *json*-а відправляються у вихідний потік.

### 4. Прийом даних
За прийом кінцевих даних, як і раніше, відповідає скрипт *consumer.py*. Він створює об'єкт *KafkaConsumer* та підписується на вихідний топік для алертів. 

![alt text](https://github.com/EKorohodov-MS-CS-DSDA/goit-de-hw-06/blob/main/p2_Screenshot.jpg)
